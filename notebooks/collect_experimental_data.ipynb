{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cthb8O3LCPM1"
      },
      "source": [
        "# Experiment Data Collections Notebook\n",
        "\n",
        "This notebook generates g-values and perplexities so that you can save for later interpretation. This is what we use for our experiments with the Frequentist scoring function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be-I0MNRbyWT"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq7hChW8njFo"
      },
      "outputs": [],
      "source": [
        "# @title Install and import the required Python packages\n",
        "#\n",
        "# @markdown Running this cell may require you to restart your session.\n",
        "\n",
        "! pip install synthid-text[notebook]\n",
        "\n",
        "from collections.abc import Sequence\n",
        "import enum\n",
        "import gc\n",
        "\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "from synthid_text import detector_mean\n",
        "from synthid_text import logits_processing\n",
        "from synthid_text import synthid_mixin\n",
        "from synthid_text import detector_bayesian\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import tqdm\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Hugging Face Hub\n",
        "huggingface_hub.notebook_login()"
      ],
      "metadata": {
        "id": "O-oFxl9Dsbg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w9a5nANolFS_"
      },
      "outputs": [],
      "source": [
        "# @title Choose your model.\n",
        "\n",
        "# @markdown Edit this cell to set your pre-trained model name, where your parameters will come from, and model class. AutoModel likely will not work\n",
        "MODEL_NAME = 'meta-llama/Llama-3.1-8B-Instruct'\n",
        "MODEL_CLASS = transformers.LlamaForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_pe-hG6SW6H"
      },
      "outputs": [],
      "source": [
        "# @title Configure your device\n",
        "#\n",
        "# @markdown Its important that your model fits in GPU memory. If you use a very small model like GPT-2, you might be able to get away with using CPU. We used A100 for most experiments\n",
        "\n",
        "DEVICE = (\n",
        "    torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        ")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOGvCjyVjjQ5"
      },
      "outputs": [],
      "source": [
        "# @title Example watermarking config\n",
        "#\n",
        "# @markdown We use the defualt configuration, with m=30 tournament layers and slidingwindow size H=4, context history of 1024 tokens for context masking\n",
        "\n",
        "CONFIG = synthid_mixin.DEFAULT_WATERMARKING_CONFIG\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79mekKj5UUZR"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the required constants, tokenizer, and logits processor\n",
        "\n",
        "# Feel free to mess around with these hyperparams\n",
        "OUTPUTS_LEN = 1024\n",
        "TEMPERATURE = 0.5\n",
        "TOP_K = 40\n",
        "TOP_P = 0.99\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "logits_processor = logits_processing.SynthIDLogitsProcessor(\n",
        "    **CONFIG, top_k=TOP_K, temperature=TEMPERATURE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hndT3YCQUt6D"
      },
      "outputs": [],
      "source": [
        "# @title Utility functions to load models, compute perplexity, and process prompts.\n",
        "\n",
        "def load_model(\n",
        "    model_name: str,\n",
        "    expected_device: torch.device,\n",
        "    enable_watermarking: bool = False,\n",
        ") -> transformers.PreTrainedModel:\n",
        "  class SynthIDModelClass(synthid_mixin.SynthIDSparseTopKMixin, MODEL_CLASS):\n",
        "    pass\n",
        "  model_cls = SynthIDModelClass if enable_watermarking else MODEL_CLASS\n",
        "  model = model_cls.from_pretrained(\n",
        "      model_name,\n",
        "      device_map='auto',\n",
        "      torch_dtype=torch.bfloat16,\n",
        "  )\n",
        "\n",
        "  if str(model.device) != str(expected_device):\n",
        "    raise ValueError('Model device not as expected.')\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def _compute_perplexity(\n",
        "    outputs: torch.LongTensor,\n",
        "    scores: torch.FloatTensor,\n",
        "    eos_token_mask: torch.LongTensor,\n",
        "    watermarked: bool = False,\n",
        ") -> float:\n",
        "  \"\"\"Compute perplexity given the model outputs and the logits.\"\"\"\n",
        "  len_offset = len(scores)\n",
        "  if watermarked:\n",
        "    nll_scores = scores\n",
        "  else:\n",
        "    nll_scores = [\n",
        "        torch.gather(\n",
        "            -torch.log(torch.nn.Softmax(dim=1)(sc)),\n",
        "            1,\n",
        "            outputs[:, -len_offset + idx, None],\n",
        "        )\n",
        "        for idx, sc in enumerate(scores)\n",
        "    ]\n",
        "  nll_sum = torch.nan_to_num(\n",
        "      torch.squeeze(torch.stack(nll_scores, dim=1), dim=2)\n",
        "      * eos_token_mask.long(),\n",
        "      posinf=0,\n",
        "  )\n",
        "  nll_sum = nll_sum.sum(dim=1)\n",
        "  nll_mean = nll_sum / eos_token_mask.sum(dim=1)\n",
        "  return nll_mean.sum(dim=0)\n",
        "\n",
        "\n",
        "def _process_raw_prompt(prompt: Sequence[str]) -> str:\n",
        "  \"\"\"Add chat template to the raw prompt.\"\"\"\n",
        "  return tokenizer.apply_chat_template(\n",
        "      [{'role': 'user', 'content': prompt.decode().strip('\"')}],\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load ELI5 dataset\n",
        "\n",
        "eli5_prompts = datasets.load_dataset(\"Pavithree/eli5\")"
      ],
      "metadata": {
        "id": "t292_wu7HQVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate g-values scores on ELI5 test\n",
        "\n",
        "#@markdown We use 100 samples from ELI5, using the maximum batch size that we have enough memory for. You will likely need to play around with these.\n",
        "NUM_BATCHES = 20 # @param {\"type\":\"integer\"}\n",
        "BATCH_SIZE = 5 # @param {\"type\":\"integer\"}\n",
        "ENABLE_WATERMARKING = True # @param {\"type\":\"boolean\"}\n",
        "\n",
        "model = load_model(MODEL_NAME, expected_device=DEVICE, enable_watermarking=ENABLE_WATERMARKING)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "eli5_g_values = []\n",
        "eli5_combined_mask = []\n",
        "eli5_perplexities = []\n",
        "for batch_id in tqdm.tqdm(range(NUM_BATCHES)):\n",
        "  prompts = eli5_prompts['test']['title'][\n",
        "      batch_id * BATCH_SIZE:(batch_id + 1) * BATCH_SIZE]\n",
        "  prompts = [_process_raw_prompt(prompt.encode()) for prompt in prompts]\n",
        "  inputs = tokenizer(\n",
        "      prompts,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "  ).to(DEVICE)\n",
        "  _, inputs_len = inputs['input_ids'].shape\n",
        "\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      do_sample=True,\n",
        "      max_length=inputs_len + OUTPUTS_LEN,\n",
        "      temperature=TEMPERATURE,\n",
        "      top_k=TOP_K,\n",
        "      top_p=TOP_P,\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "\n",
        "  scores = outputs.scores\n",
        "  outputs = outputs.sequences\n",
        "\n",
        "  eos_token_mask = logits_processor.compute_eos_token_mask(\n",
        "      input_ids=outputs[:, inputs_len:],\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "\n",
        "  eli5_perplexities.append(_compute_perplexity(outputs, scores, eos_token_mask, watermarked=ENABLE_WATERMARKING).cpu())\n",
        "\n",
        "  eos_token_mask = eos_token_mask[:, CONFIG['ngram_len'] - 1 :]\n",
        "\n",
        "  context_repetition_mask = logits_processor.compute_context_repetition_mask(\n",
        "      input_ids=outputs[:, inputs_len:],\n",
        "  )\n",
        "\n",
        "  combined_mask = context_repetition_mask * eos_token_mask\n",
        "\n",
        "  g_values = logits_processor.compute_g_values(\n",
        "      input_ids=outputs[:, inputs_len:],\n",
        "  )\n",
        "\n",
        "  eli5_g_values.append(g_values.cpu())\n",
        "  eli5_combined_mask.append(combined_mask.cpu())\n",
        "\n",
        "  del inputs, prompts, eos_token_mask, context_repetition_mask, combined_mask, g_values, outputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def cat(l):\n",
        "  max_len=max([val.shape[1] for val in l])\n",
        "  return torch.cat([torch.nn.functional.pad(val, (0, 0, 0, max_len-val.shape[1]) if len(val.shape) == 3 else (0, max_len-val.shape[1]), mode=\"constant\", value=(tokenizer.eos_token_id if len(val.shape) == 3 else False)) for val in l])\n",
        "\n",
        "padded_eli5_g_values = cat(eli5_g_values)\n",
        "padded_eli5_combined_mask = cat(eli5_combined_mask)\n"
      ],
      "metadata": {
        "id": "GIR1szWCJt6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save results to files\n",
        "F_MODEL_NAME = MODEL_NAME.replace(\"/\",\"_\")\n",
        "torch.save(padded_eli5_g_values, f\"eli5_g_values_{F_MODEL_NAME}_t={TEMPERATURE}_{'wm' if ENABLE_WATERMARKING else 'uwm'}.pt\")\n",
        "torch.save(padded_eli5_combined_mask, f\"eli5_combined_mask_{F_MODEL_NAME}_t={TEMPERATURE}_{'wm' if ENABLE_WATERMARKING else 'uwm'}.pt\")\n",
        "torch.save(eli5_perplexities, f\"eli5_perplexities_{F_MODEL_NAME}_t={TEMPERATURE}_{'wm' if ENABLE_WATERMARKING else 'uwm'}.pt\")"
      ],
      "metadata": {
        "id": "BdjYkCkeTTgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}